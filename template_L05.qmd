---
title: "L05 Feature Engineering II"
subtitle: "Data Science 3 with R (STAT 301-3)"
author: "YOUR NAME"
pagetitle: "L05 YOUR NAME"
date: today

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    code-fold: false
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji
reference-location: margin
citation-location: margin
---


::: {.callout-tip icon=false}

## Github Repo Link

To link to your github **repo**sitory, appropriately edit the example link below. Meaning replace `https://your-github-repo-url` with your github repo url. Suggest verifying the link works before submitting.

[https://your-github-repo-url](https://your-github-repo-url)

:::


## Overview

The goal of this lab is to (1) expand our feature engineering understanding and (2) introduce a new model type: **M**ultivariate **A**daptive **R**egression **S**plines (MARS).


## Instructions

We will be utilizing `wildfires.csv` dataset contained in the data subdirectory. `wildfires_codebook.html` provides a quick overview of the data which is where students should begin.

Useful Readings:

- [TMWR Chapter 16 Dimensionality Reduction](https://www.tmwr.org/dimensionality)
- [TMWR Bookclub notes](https://r4ds.github.io/bookclub-tmwr/dimensionality-reduction.html)


## Exercise

::: {.callout-note icon="false"}
## Prediction goal

Our goal is to predict whether or not a wildfire will reach it (`wlf`) given all the other variables in our dataset except for `burned`.

:::

### Exercise 1

Name 2 model types that are sensitive to skewed predictors or predictors with extreme/unusual values (i.e. outliers).

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Are there any model types that are immune to such issues with their predictors? If so, name one and explain why it is immune to outliers.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::


### Exercise 2

When is a standardization process (think scaling) essential? Provide an example of when it is essential.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

### Exercise 3

Name 2 model types that are adversely affected by highly correlated predictor variables. Name two methods that could be used to help with this issue --- identify the `step_*()` functions that implement the identified methods.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

### Exercise 4

Prepare your data for modeling. For tuning we suggest using 5 folds and 3 repeats.

#### Task 1

Create a "minimal effort/kitchen sink" recipe that predicts `wlf` using all variables except for `burned`.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE - display recipe

:::


#### Task 2

Create a recipe with more advanced feature engineering that predicts `wlf` using all variables except for `burned` and the following steps

  - use a Yeo-Johnson transformation on `windspd`
  - remove highly correlated variables (above 90%)
  - add interactions between all predictor variables
  - add a principal component step (could try kernel pca as well) in order to reduce the dimensionality after adding all two-way interactions. Tune the number of components as well by flagging it with `tune()`. 


::: {.callout-note}
In several feature engineering steps there are often parameters that must be chosen. For example the `deg_free` in a `step_ns()` transformation or the number of components (`num_comp`) in a `step_pca()`. Rather than "guess" the best setting for them, it is often better to tune these hyperparameters. 
:::
  
::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE - display recipe here

:::

### Exercise 5

Train 2 MARS models. One model should use the recipe created in Exercise 4 Task 1 and the other model should use the recipe created in Exercise 4 Task 2. There should be no other differences between these two models.

- Model 1 (kitchen sink): tune `num_terms` and `prod_degree`
- Model 2 (more feature engineering): tune `num_terms`, `prod_degree`, and `num_comp`

::: {.callout-note}
Determining the appropriate grids for tuning requires a solid understanding of each step and what the modeling process will be doing.

For the kitchen sink we will be leveraging the fact that MARS will be automatically searching non-linear relationships though interactions and splines. So a high number of terms makes sense. 

For the model utilizing more featuring engineering, we first include all two-way interactions (expand the predictor space -- number of predictor columns) and then use principal components to reduce the predictor space. Doing the expansion prior to making the principal components is done in hopes of capturing non-linear relationship information in the components. This means we don't need many terms in the MARS model, say no more than 5, and we don't want too many principal components, say no more than 10. 
:::

Jointly tuning a recipe step and model specification parameters requires a slight adjustment to building the tuning grid. You will need to `extract_parameter_set_dials` from the recipe and from the model. You then need to use `bind_rows()` to combine the output so that all parameters to be tuned are in one object. Then you can construct a grid that contains all parameters flagged for tuning.


::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE - display code for engine and grid used for the feature engineered recipe

:::


What were the optimal tuning parameters for each model?

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::

Does one of your recipes result in a significantly better model? Be sure to clearly state the metric chosen, the metric mean, standard error, and the run time of each model.

::: {.callout-tip icon="false"}
## Solution

YOUR SOLUTION HERE

:::
